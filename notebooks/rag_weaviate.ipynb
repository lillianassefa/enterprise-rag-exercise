{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "weaviate_api_key = os.getenv(\"WEAVIATE_API_kEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classes': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = weaviate.Client(\n",
    "    url=\"https://precision-rag-ov8u665q.weaviate.network\", \n",
    "    auth_client_secret=weaviate.AuthApiKey(api_key= weaviate_api_key),\n",
    "     additional_headers={\n",
    "        \"X-OpenAI-Api-Key\": openai_api_key  \n",
    "    }\n",
    "      )\n",
    "\n",
    "client.schema.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.schema.delete_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"TenAcademyChallengDocument\"\n",
    "\n",
    "class_obj = {\n",
    "    \"class\": collection_name,\n",
    "    \"description\": \"Represents articles related to AI-driven solutions and language models.\",\n",
    "    \"properties\": [\n",
    "        {\n",
    "            \"name\": text_content,\n",
    "            \"dataType\": [\"text\"],\n",
    "            \"description\": \"The title of the article.\",\n",
    "        },\n",
    "        # {\n",
    "        #     \"name\": \"content\",\n",
    "        #     \"dataType\": [\"text\"],\n",
    "        #     \"description\": \"The content of the article.\",\n",
    "        # },\n",
    "    ],\n",
    "    \"vectorizer\": \"text2vec-openai\",  \n",
    "    \"vectorIndexType\": \"flat\",\n",
    "    \"vectorIndexConfig\": {\n",
    "        \"bq\": {\n",
    "            \"enabled\": True, \n",
    "            \"rescoreLimit\": 200,  \n",
    "            \"cache\": True,  # e\n",
    "        },\n",
    "        \"vectorCacheMaxObjects\": 100000,  \n",
    "   \"moduleConfig\": {\n",
    "    \"generative-openai\": {\n",
    "        \"model\": \"text-davinci-003\",  \n",
    "        \"temperature\": 0.7, \n",
    "        \"maxTokens\": 200,  \n",
    "    }\n",
    "}\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "client.schema.update_config(collection_name, class_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classes': [{'class': 'TenAcademyChallengDocument',\n",
       "   'description': 'Represents articles related to AI-driven solutions and language models.',\n",
       "   'invertedIndexConfig': {'bm25': {'b': 0.75, 'k1': 1.2},\n",
       "    'cleanupIntervalSeconds': 60,\n",
       "    'stopwords': {'additions': None, 'preset': 'en', 'removals': None}},\n",
       "   'moduleConfig': {'text2vec-openai': {'baseURL': 'https://api.openai.com',\n",
       "     'model': 'ada',\n",
       "     'modelVersion': '002',\n",
       "     'type': 'text',\n",
       "     'vectorizeClassName': True}},\n",
       "   'multiTenancyConfig': {'enabled': False},\n",
       "   'properties': [{'dataType': ['text'],\n",
       "     'description': 'The title of the article.',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'title',\n",
       "     'tokenization': 'word'},\n",
       "    {'dataType': ['text'],\n",
       "     'description': 'The content of the article.',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'content',\n",
       "     'tokenization': 'word'}],\n",
       "   'replicationConfig': {'factor': 1},\n",
       "   'shardingConfig': {'virtualPerPhysical': 128,\n",
       "    'desiredCount': 1,\n",
       "    'actualCount': 1,\n",
       "    'desiredVirtualCount': 128,\n",
       "    'actualVirtualCount': 128,\n",
       "    'key': '_id',\n",
       "    'strategy': 'hash',\n",
       "    'function': 'murmur3'},\n",
       "   'vectorIndexConfig': {'distance': 'cosine',\n",
       "    'vectorCacheMaxObjects': 100000,\n",
       "    'pq': {'enabled': False, 'rescoreLimit': -1, 'cache': False},\n",
       "    'bq': {'enabled': True, 'rescoreLimit': 200, 'cache': True}},\n",
       "   'vectorIndexType': 'flat',\n",
       "   'vectorizer': 'text2vec-openai'}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.schema.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('context.txt','r') as file:\n",
    "    text_content = file.read()\n",
    "    # print(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"TenAcademyChallengDocument\"\n",
    "\n",
    "class_obj = {\n",
    "    \"class\": collection_name,\n",
    "    \"description\": \"Represents articles related to AI-driven solutions and language models.\",\n",
    "    \"properties\": [\n",
    "        {\n",
    "            \"name\": \"content\",\n",
    "            \"dataType\": [\"text\"],\n",
    "            \"description\": \"content\",\n",
    "        },\n",
    "    ],\n",
    "    \"vectorizer\": \"text2vec-openai\",  \n",
    "    \"vectorIndexType\": \"flat\",\n",
    "    \"vectorIndexConfig\": {\n",
    "        \"bq\": {\n",
    "            \"enabled\": True, \n",
    "            \"rescoreLimit\": 200,  \n",
    "            \"cache\": True,  # e\n",
    "        },\n",
    "        \"vectorCacheMaxObjects\": 100000,  \n",
    "   \"moduleConfig\": {\n",
    "    \"generative-openai\": {\n",
    "        \"model\": \"text-davinci-003\",  \n",
    "        \"temperature\": 0.7, \n",
    "        \"maxTokens\": 200,  \n",
    "    }\n",
    "}\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "client.schema.create_class(class_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_object = {\n",
    "    \"class\": collection_name,\n",
    "    \"properties\": {\n",
    "        \"content\": text_content\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.util import generate_uuid5\n",
    "\n",
    "uuid = client.data_object.create(\n",
    "    class_name= collection_name,\n",
    "    data_object= data_object,\n",
    "    uuid=generate_uuid5(data_object)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'Get': {'TenAcademyChallengDocument': [{'content': None}, {'content': 'Business objective  \\nPromptlyTech is an innovative e-business specializing in providing AI-driven solutions for optimizing the use of Language Models (LLMs) in various industries. The company aims to revolutionize how businesses interact with LLMs, making the technology more accessible, efficient, and effective. By addressing the challenges of prompt engineering, the company plays a pivotal role in enhancing decision-making, operational efficiency, and customer experience across various industries. PromptlyTech\\'s solutions are designed to cater to the evolving needs of a digitally-driven business landscape, where speed and accuracy are key to staying competitive.\\nThe company focuses on key services: Automatic Prompt Generation, Automatic Evaluation Data Generation, and Prompt Testing and Ranking.\\n1. Automatic Prompt Generation Service:\\nThis service streamlines the process of creating effective prompts, enabling businesses to efficiently utilize LLMs for generating high-quality, relevant content. It significantly reduces the time and expertise required in crafting prompts manually.\\n2. Automatic Evaluation Data Generation Service:\\nPromptlyTech’s service automates the generation of diverse test cases, ensuring comprehensive coverage and identifying potential issues. This enhances the reliability and performance of LLM applications, saving significant time in the QA(Quality Assurance) process.\\n3. Prompt Testing and Ranking Service:\\nPromptlyTech’s service evaluates and ranks different prompts based on effectiveness, helping Users to get the desired outcome from LLM. It ensures that chatbots and virtual assistants provide accurate, contextually relevant responses, thereby improving user engagement and satisfaction.\\n\\nBackground Context\\n\\nIn the evolving field of artificial intelligence, Language Models (LLMs) like GPT-3.5 and GPT-4 have become crucial for various applications. Their effectiveness, however, heavily depends on the quality of the prompts they receive, leading to the emergence of \"prompt engineering\" as a key skill.\\n\\nPrompt engineering is the craft of designing queries or statements to guide LLMs to produce desired outcomes. The challenge lies in the sensitivity of these models to prompt nuances, where slight variations can yield vastly different results. This poses a significant hurdle for users, especially in business contexts where accuracy and relevance are paramount.\\n\\nThe need for simplified, efficient prompt engineering is clear. Automating and optimizing this process can save time, enhance LLM productivity, and make advanced AI capabilities more accessible to a broader range of users. The tasks of Automatic Prompt Generation, Evaluation Data Generation, and Prompt Testing and Ranking are aimed at addressing these challenges, streamlining the prompt engineering process for more effective use of LLMs.\\nLearning Outcomes\\nSkills Development\\nPrompt Engineering Proficiency: Gain expertise in crafting effective prompts that guide LLMs to desired outputs, understanding nuances and variations in language that impact model responses.\\nCritical Analysis: Develop the ability to critically analyze and evaluate the effectiveness of different prompts based on their performance in varied scenarios.\\nTechnical Aptitude with LLMs: Enhance technical skills in using advanced language models like GPT-4 and GPT-3.5-Turbo, understanding their functionalities and capabilities.\\nProblem-Solving and Creativity: Cultivate creative problem-solving skills by generating innovative prompts and test cases, addressing complex and varied objectives.\\nData Interpretation: Learn to interpret and analyze data from test cases and prompt evaluations, deriving meaningful insights from performance metrics.\\n\\nKnowledge Acquisition\\nUnderstanding of Language Models: Acquire a deeper understanding of how LLMs function, including their strengths, limitations, and the principles behind their responses.\\nInsights into Automated Evaluation Data Generation: Gain knowledge about the methodology and importance of creating test cases for evaluating prompt effectiveness.\\nELO Rating System and its Applications: Learn about the ELO rating system used for ranking prompts, understanding its mechanics and relevance in performance evaluation.\\nPrompt Optimization Strategies: Understand various strategies for refining and optimizing prompts to achieve better alignment with specific goals and desired outcomes.\\nIndustry Best Practices: Familiarize with the best practices in prompt engineering within different industries, learning about real-world applications and challenges.\\n\\nTeam\\nTutors: \\nYabebal\\nEmitinan\\nRehmet\\nBadges\\nEach week, one user will be awarded one of the badges below for the best performance in the category below.\\n\\nIn addition to being the badge holder for that badge, each badge winner will get +20 points to the overall score.\\n\\nVisualization - quality of visualizations, understandability, skimmability, choice of visualization\\nQuality of code - reliability, maintainability, efficiency, commenting - in future this will be CICD/CML\\nInnovative approach to analysis -using latest algorithms, adding in research paper content and other innovative approaches\\nWriting and presentation - clarity of written outputs, clarity of slides, overall production value\\nMost supportive in the community - helping others, adding links, tutoring those struggling\\n\\nThe goal of this approach is to support and reward expertise in different parts of the Machine learning engineering toolbox.\\n\\nGroup Work Policy\\nEveryone has to submit all their work individually. \\nInstruction: Automatic Prompt Engineering\\nFundamental Tasks\\nThe core tasks for this week’s challenge in Automatic Prompt Engineering are outlined below:\\n\\nUnderstand Prompt Engineering Tools and Concepts: Gain a thorough understanding of the tools and theoretical concepts involved in prompt engineering for Language Models (LLMs).\\n\\nFamiliarize with Language Models: Learn about the capabilities and functionalities of advanced LLMs like GPT-4 and GPT-3.5-Turbo.\\n\\nDevelop a Plan for Prompt Generation and Testing: Create a comprehensive plan that outlines the approach for automated prompt generation, test case creation, and prompt evaluation.\\n\\nSet Up a Development Environment: Prepare a suitable development environment that supports the integration and testing of LLMs in the prompt engineering process.\\n\\nDesign User Interface for Prompt System: Plan and initiate the development of a user-friendly interface for prompt input, refinement, and performance analysis.\\n\\nPlan Integration of LLMs: Strategize the integration of LLMs into the prompt system for automated generation and testing.\\n\\nBuild and Refine Prompt Generation System: Develop the automated prompt generation system, ensuring it aligns with user inputs and objectives.\\n\\nDevelop Automatic Evaluation Data Generation System: Create a system for generating test cases that evaluate the effectiveness of prompts in various scenarios.\\n\\nImplement Prompt Testing and Evaluation Mechanism: Set up testing procedures using Monte Carlo matchmaking and ELO rating systems to evaluate and rank prompts.\\n\\nRefine and Optimize System Based on Feedback: Continuously refine the prompt generation and evaluation system based on user feedback and performance data.\\n\\n\\n\\n\\n\\n\\n\\nTask 1: Review the Evolution of Automatic Prompt Engineering\\nFocus on understanding the key developments in the field of automatic prompt engineering for Language Models (LLMs).\\n\\nStudy Key Concepts and Tools:\\nUnderstand the key components of an enterprise grade RAG systems\\nRetrieval-augmented generation (RAG): What it is and why it’s a hot topic for enterprise AI\\nAdvanced RAG for LLMs/SLMs\\nRAG for Text Generation Processes in Businesses (check part 1, 3, & 4 as well) \\nLangchain Reterivers\\nUnderstand the need for advanced prompt engineering in building enterprise grade RAG systems\\nFull Fine-Tuning, PEFT, Prompt Engineering, and RAG: Which One Is Right for You?\\nAdvanced Prompt Engineering - Practical Examples\\nPrompt Engineering 201: Advanced methods and toolkits\\nDo you agree with this article? RAG is Just Fancier Prompt Engineering\\nUnderstand the need for evaluating RAG components\\nAn Overview on RAG Evaluation\\nEvaluating RAG: Using LLMs to Automate Benchmarking of Retrieval Augmented Generation Systems\\nEvaluating RAG Applications with RAGAs\\nRAG Evaluation Using LangChain and Ragas\\nRAG System: Metrics and Evaluation Analysis with LlamaIndex\\nEvaluating RAG Part I: How to Evaluate Document Retrieval\\nEvaluating RAG/LLMs in highly technical settings using synthetic QA generation\\nEvaluating Multi-Modal RAG\\nUnderstand the tools and techniques to automatically generate RAG evaluation data \\nThe Tech Buffet #16: Quickly Evaluate your RAG Without Manually Labeling Test Data\\nGenerating a Synthetic Dataset for RAG\\n\\n\\nLearn key packages to planning, building, testing, monitoring, and deploying enterprise grade RAG system\\nIterate on LLMs faster: Measure LLM quality and catch regressions\\nBuilding RAG-based LLM Applications for Production\\nARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems\\nUnderstand the end-to-end technology stack of RAG systems\\nEnd-to-End LLMOps Platform\\nAn Enterprise-Grade Reference Architecture for the Production Deployment of LLMs Using the RAG Pattern on Azure OpenAI\\nTask 2: Design and Develop the Prompt Generation System\\nUsers can input a description of their objective or task and specify a few scenarios along with their expected outputs. \\nWrite or adopt sophisticated algorithms, you generate multiple prompt options based on the provided information. \\nThis automated prompt generation process saves time and provides a diverse range of alternatives to consider. But add an evaluation metrics that check whether the generated prompt candidate aligns with the input description.\\nTask 3: Implement Evaluation Data Generation and Evaluation\\nTo further enhance the prompt generation process, incorporate automatic Evaluation Data Generation. \\nBy analysing the description provided by the user,  create a set of test cases that serve as evaluation benchmarks for the prompt candidates.\\nThese test cases simulate various scenarios, enabling users to observe how each prompt performs in different contexts. \\nThe generated test cases serve as a starting point, sparking creativity and inspiring additional test cases for comprehensive evaluation.\\nTask 4 : Prompt Testing and Ranking\\nGoals\\nComprehensive Evaluation: Provide a robust system that uses various methodologies for a thorough assessment of prompts.\\nCustomizable and User-Centric: Allow users to choose or customize their preferred evaluation methods.\\nDynamic and Adaptive: Ensure the system remains flexible and adaptive, capable of incorporating new ranking methodologies as they emerge.\\n\\nPrimary Methods\\n\\nMonte Carlo Matchmaking: This method is used to select and match different prompt candidates against each other. The Monte Carlo method, known for its applications in problem-solving and decision-making processes, helps in optimizing the information gained from each prompt battle. By simulating various matchups, it allows the system to test the effectiveness of each prompt in different scenarios.\\nELO Rating System:  This system, which is commonly used in chess and other competitive games, rates the prompts based on their performance in the battles. Each prompt candidate is assigned a rating that reflects its success in previous matchups. The system takes into account not just the number of wins but also the strength of the opponents each prompt has defeated. This rating helps in objectively ranking the prompts based on their effectiveness.\\n\\nAdditional Ranking and Matching Mechanisms\\nTrueSkill Rating System: Ideal for scenarios involving multiple competitors, adjusting ratings based on not just wins and losses but also the uncertainty in performance.\\nGlicko Rating System: Similar to ELO but with added flexibility, accounting for the volatility in a player\\'s (or prompt’s) performance and the reliability of their rating.\\nBayesian Rating Systems: Applies Bayesian inference for a probabilistic approach to rating, considering uncertainties and contextual variations in prompt performance.\\nPairwise Comparison Methods: Involves direct comparisons between pairs of prompts, potentially integrating user preferences or expert evaluations into the ranking process.\\nCategorical Ranking: Instead of a numerical rating, prompts are categorized based on performance criteria like creativity, relevance, etc., for more qualitative assessments.\\nAdaptive Ranking Algorithms: Algorithms that learn and adjust over time, considering historical performance data and evolving user preferences or requirements.\\nSemantic Similarity Matching: Using NLP techniques to match prompts based on semantic content, ideal for understanding nuanced differences in prompt effectiveness.\\n\\nYou should adopt an innovative approach to prompt evaluation by utilizing Monte Carlo matchmaking and  ELO rating systems, or any alternative method to match and rank.\\nTask 5: User Interface Development\\nDevelop a user-friendly interface for interacting with the prompt engineering system.\\nUI Design: Plan and design a user interface that allows users to easily input data, receive prompts, and view evaluation results.\\nUI Implementation: Develop and integrate the user interface with the backend prompt engineering system.\\nTask 6: System Integration and Testing\\nIntegrate all components of the system and conduct comprehensive testing.\\nIntegrate the prompt generation, Evaluation Data Generation, evaluation, and user interface components.\\nTest the entire system for functionality, usability, and performance. Refine based on feedback and test results.\\n\\n\\nTutorials Schedule\\nIn the following, the colour purple indicates morning sessions, and blue indicates afternoon sessions.\\nMonday: Understanding Prompt engineering \\nHere the trainees will understand the week’s challenge.\\nIntroduction to Week Challenge (Yabebal)\\nIntroduction and challenge to prompt engineering (Fikerte)\\n\\n\\nKey Performance Indicators:\\n\\n\\nUnderstanding week’s challenge\\nUnderstanding the prompt engineering\\nAbility to reuse previous knowledge\\nTuesday\\nRAG components (Rehmet)\\nTechniques to improving R (Retrievers) in RAG (Emitnan)\\n\\n\\nKey Performance Indicators:\\n\\n\\nUnderstanding Prompt ranking \\nUnderstanding prompt matching \\nAbility to reuse previous knowledge\\n\\n\\nWednesday\\nRAG Evaluation Data Generation (Abel)\\nUnderstanding of prompt matching and ranking (Mahlet)\\nThursday\\nRAG evaluation metrics (Emitnan)\\nRAGObs - DevObs of RAG development and production deployment  \\n\\nDeliverables\\nNOTE: Document should be a PDF stored in google drive or published blog link. DO NOT SUBMIT A LINK as PDF! If you want to submit pdf document, it should be the content of your report not a link. \\nInterim Submission - Wednesday 8pm UTC\\nLink to your code in GitHub\\nRepository where you will be using to complete the tasks in this week\\'s challenge. A minimum requirement is that you have a well structured repository and some coding progress is made.\\n\\n\\nA review report of your reading and understanding of Task 1 and any progress you made in other tasks. \\nFeedback\\nYou may not receive detailed comments on your interim submission, but will receive a grade.\\nFinal Submission - Saturday 8pm UTC\\nLink to your code in GitHub \\nComplete work  for Automatic prompt generation\\nComplete work  for Automatic evaluation \\nComplete work for Evaluation Data Generation\\n\\nA blog post entry (which you can submit for example to Medium publishing) or a pdf report. . \\nFeedback\\nYou will receive comments/feedback in addition to a grade.\\n\\n\\n\\n'}]}}}\n"
     ]
    }
   ],
   "source": [
    "response = (\n",
    "    client.query.get(class_name=collection_name, properties=[\"content\"])\n",
    "    .with_near_text({\"concepts\": [\"bussiness\"]})\n",
    "    .with_limit(3)\n",
    "    .do()\n",
    ")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_file_path = 'response3.json'\n",
    "\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(response, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
